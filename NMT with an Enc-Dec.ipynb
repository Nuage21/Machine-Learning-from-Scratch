{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation with Bidirectional Encoder-Decoder LSTMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    [\"I love you\", \"je t'aime\"],\n",
    "    [\"What is your name ?\", \"Comment tu t'appele ?\"],\n",
    "    [\"How old are you\", \"Quel age as tu\"],\n",
    "    [\"why is the world so dumb\", \"pourquoi le monde est si idiot\"],\n",
    "    [\"I found the dog\", \"j'ai trouve le chien\"],\n",
    "    [\"He died\", \"Il est mort\"],\n",
    "    [\"I kissed him\", \"Je l'ai embrasse\"],\n",
    "    [\"New-York is a beautiful city\", \"New-York est une belle ville\"],\n",
    "    [\"what are you doing\", \"Que ce que tu fait\"],\n",
    "    [\"I lost hope\", \"J'ai perdu l'espoir\"],\n",
    "    [\"Donald Trump is the worst president\", \"Donald Trump est le pire president\"],\n",
    "    [\"Do you feel bad\", \"Tu te sens mauvais\"],\n",
    "    [\"I found an apple at my desk\", \"J'ai trouve une pomme sur mon bureau\"],\n",
    "    [\"You broke the mug\", \"T'as casse le mug\"],\n",
    "    [\"Do you hate me ?\", \"Tu me deteste ?\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing:\n",
    "---\n",
    "Before we jump in to our model, a preprocessing part is a must."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Similar Words Tokenization:\n",
    "There's no gain in having 3 word vectors for New-York, Havana and Tokyo, they're all city names, the same for America, Germany and Russia being country names. The dataset would be huge if we include training exemples for each city name and our model will learn better if all he knows about those words is their category appartenance, this way, even if we do not have a training exemple for \"I went to Tokyo\", our model having already been trained on \"I went to Paris\" will know how to translate the former.\n",
    "## 2. Sequence Begin & End Tokens:\n",
    "The Sequence Begin token and the Encoding vector are fed to the decoder to trigger translation, the end sequence token tells us when the translation is over\n",
    "## 3. Tokenization and Vocab Dictionnary\n",
    "A vocab dictionnary mapping each word to its woken and another mapping each token to its word are needed to transform our training data into a valid input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tokens = {\n",
    "    'city_token': '<city>',\n",
    "    'country_token': '<country>',\n",
    "    'person_token': '<person>'\n",
    "}\n",
    "\n",
    "custom_words = {\n",
    "    'new-york': 'city_token',\n",
    "    'tokyo': 'city_token',\n",
    "    'paris': 'city_token',\n",
    "    'donald': 'person_token',\n",
    "    'trump': 'person_token',\n",
    "    'hakim': 'person_token',\n",
    "    'andrew': 'person_token',\n",
    "    'france': 'country_token',\n",
    "    'mexico': 'country_token',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define some helper functions to de/tokenize words and create the vocabs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<unknown>': 0, '<city>': 1, '<country>': 2, '<person>': 3, '<start>': 4, '<end>': 5, '<pad>': 6, 'i': 7, 'love': 8, 'you': 9, 'what': 10, 'is': 11, 'your': 12, 'name': 13, '?': 14, 'how': 15, 'old': 16, 'are': 17, 'why': 18, 'the': 19, 'world': 20, 'so': 21, 'dumb': 22, 'found': 23, 'dog': 24, 'he': 25, 'died': 26, 'kissed': 27, 'him': 28, 'new-york': 29, 'a': 30, 'beautiful': 31, 'city': 32, 'doing': 33, 'lost': 34, 'hope': 35, 'donald': 36, 'trump': 37, 'worst': 38, 'president': 39, 'do': 40, 'feel': 41, 'bad': 42, 'an': 43, 'apple': 44, 'at': 45, 'my': 46, 'desk': 47, 'broke': 48, 'mug': 49, 'hate': 50, 'me': 51}\n",
      "---\n",
      "{'<unknown>': 0, '<city>': 1, '<country>': 2, '<person>': 3, '<start>': 4, '<end>': 5, '<pad>': 6, 'je': 7, \"t'aime\": 8, 'comment': 9, 'tu': 10, \"t'appele\": 11, '?': 12, 'quel': 13, 'age': 14, 'as': 15, 'pourquoi': 16, 'le': 17, 'monde': 18, 'est': 19, 'si': 20, 'idiot': 21, \"j'ai\": 22, 'trouve': 23, 'chien': 24, 'il': 25, 'mort': 26, \"l'ai\": 27, 'embrasse': 28, 'new-york': 29, 'une': 30, 'belle': 31, 'ville': 32, 'que': 33, 'ce': 34, 'fait': 35, 'perdu': 36, \"l'espoir\": 37, 'donald': 38, 'trump': 39, 'pire': 40, 'president': 41, 'te': 42, 'sens': 43, 'mauvais': 44, 'pomme': 45, 'sur': 46, 'mon': 47, 'bureau': 48, \"t'as\": 49, 'casse': 50, 'mug': 51, 'me': 52, 'deteste': 53}\n",
      "---\n",
      "12\n",
      "---\n",
      "[7, 8, 9, 6, 6, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "class TranslationVocab:\n",
    "    def __init__(self):\n",
    "        self.ov = self.generate_empty_vocab() # original_vocab\n",
    "        self.tv = self.generate_empty_vocab() # target_vocab\n",
    "        self.ov_size = len(self.ov)\n",
    "        self.tv_size = len(self.tv)\n",
    "        self.o_max = 0 # original phrases data maximum sequence\n",
    "        self.t_max = 0 # target phrases data maximum sequence\n",
    "\n",
    "    def load_from_dataset(self, dataset):\n",
    "        # dataset is a list of lists, each inner list holds 2 strings ['original', 'translation']\n",
    "        for sample in dataset:\n",
    "            [original, target] = sample\n",
    "            o_splitted = original.lower().strip().split(' ')\n",
    "            t_splitted = target.lower().strip().split(' ')\n",
    "            \n",
    "            # loop over original phrase\n",
    "            for word in o_splitted:\n",
    "                self.insert_word_to_vocab(self.ov, word, self.ov_size)\n",
    "\n",
    "            # loop over target phrase\n",
    "            for word in t_splitted:\n",
    "                self.insert_word_to_vocab(self.tv, word, self.tv_size, is_target = True)\n",
    "                \n",
    "            # update max-lengths\n",
    "            self.o_max = max(len(o_splitted), self.o_max)\n",
    "            self.t_max = max(len(t_splitted) + 2, self.t_max) # +2 for <start> & <end> tokens\n",
    "            \n",
    "        return self\n",
    "            \n",
    "    def get_token_of(self, word, is_target = False): # get orginal-vocab token of\n",
    "        vocab = self.ov\n",
    "        if is_target:\n",
    "            vocab = self.tv\n",
    "        if word in vocab:\n",
    "            return vocab[word]\n",
    "        return vocab['<unknown>']\n",
    "\n",
    "    def tokenize_phrase(self, phrase, is_target = False, transformation = lambda x : '<start> ' + x + ' <end>'):\n",
    "        phrase = phrase.lower().strip()\n",
    "        if transformation is not None:\n",
    "            phrase = transformation(phrase)\n",
    "        tokenized = []\n",
    "        for word in phrase.split(' '):\n",
    "            tokenized.append(self.get_token_of(word, is_target))\n",
    "        # pad to max_len\n",
    "        max_len = self.o_max\n",
    "        if is_target:\n",
    "            max_len = self.t_max\n",
    "        padding = [self.ov['<pad>']] * (max_len - len(tokenized))\n",
    "        if len(padding) > 0:\n",
    "            return tokenized + padding\n",
    "        return tokenized\n",
    "                \n",
    "    def tokenize_dataset(self, dataset): # return numpy arrays\n",
    "        X = []\n",
    "        y = []\n",
    "        for sample in dataset:\n",
    "            [original, target] = sample\n",
    "            X.append(np.array(self.tokenize_phrase(original, transformation = None), dtype=np.float32))\n",
    "            y.append(np.array(self.tokenize_phrase(target, is_target = True), dtype=np.float32))\n",
    "        return np.array(X), np.array(y)\n",
    "            \n",
    "            \n",
    "    def insert_word_to_vocab(self, vocab, word, index, is_target = False):\n",
    "        if not word in vocab:\n",
    "            vocab[word] = index\n",
    "            if is_target:\n",
    "                self.tv_size += 1\n",
    "            else:\n",
    "                self.ov_size += 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_empty_vocab(): # later to be loaded from a file\n",
    "        return {\n",
    "            '<unknown>': 0,\n",
    "            '<city>': 1,\n",
    "            '<country>': 2,\n",
    "            '<person>': 3,\n",
    "            '<start>': 4,\n",
    "            '<end>': 5,\n",
    "            '<pad>': 6,\n",
    "        }\n",
    "\n",
    "translation_vocab = TranslationVocab().load_from_dataset(dataset)\n",
    "print(translation_vocab.ov)\n",
    "print('---')\n",
    "print(translation_vocab.tv)\n",
    "print('---')\n",
    "print(translation_vocab.get_token_of('?', True))            \n",
    "print('---')\n",
    "print(translation_vocab.tokenize_phrase('I love you', is_target = False, transformation = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training input shape = (15, 7, 1)\n",
      "training output shape = (15, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "X, y = translation_vocab.tokenize_dataset(dataset)\n",
    "X, y = np.expand_dims(X, 2), np.expand_dims(y, 2) # to get a n_samples x Sequence-Length x 1 shape\n",
    "print(f'training input shape = {X.shape}')\n",
    "print(f'training output shape = {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
