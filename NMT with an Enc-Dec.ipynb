{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation with Bidirectional Encoder-Decoder LSTMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import LSTM, Dense, Input\n",
    "from keras.utils import to_categorical as one_hot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    [\"I love you\", \"je t'aime\"],\n",
    "    [\"What is your name ?\", \"Comment tu t'appele ?\"],\n",
    "    [\"How old are you\", \"Quel age as tu\"],\n",
    "    [\"why is the world so dumb\", \"pourquoi le monde est si idiot\"],\n",
    "    [\"I found the dog\", \"j'ai trouve le chien\"],\n",
    "    [\"He died\", \"Il est mort\"],\n",
    "    [\"I kissed him\", \"Je l'ai embrasse\"],\n",
    "    [\"New-York is a beautiful city\", \"New-York est une belle ville\"],\n",
    "    [\"what are you doing\", \"Que ce que tu fait\"],\n",
    "    [\"I lost hope\", \"J'ai perdu l'espoir\"],\n",
    "    [\"Donald Trump is the worst president\", \"Donald Trump est le pire president\"],\n",
    "    [\"Do you feel bad\", \"Tu te sens mauvais\"],\n",
    "    [\"I found an apple at my desk\", \"J'ai trouve une pomme sur mon bureau\"],\n",
    "    [\"You broke the mug\", \"T'as casse le mug\"],\n",
    "    [\"Do you hate me ?\", \"Tu me deteste ?\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing:\n",
    "---\n",
    "Before we jump in to our model, a preprocessing part is a must."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Similar Words Tokenization:\n",
    "There's no gain in having 3 word vectors for New-York, Havana and Tokyo, they're all city names, the same for America, Germany and Russia being country names. The dataset would be huge if we include training exemples for each city name and our model will learn better if all he knows about those words is their category appartenance, this way, even if we do not have a training exemple for \"I went to Tokyo\", our model having already been trained on \"I went to Paris\" will know how to translate the former.\n",
    "## 2. Sequence Begin & End Tokens:\n",
    "The Sequence Begin token and the Encoding vector are fed to the decoder to trigger translation, the end sequence token tells us when the translation is over\n",
    "## 3. Tokenization and Vocab Dictionnary\n",
    "A vocab dictionnary mapping each word to its woken and another mapping each token to its word are needed to transform our training data into a valid input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_words = {\n",
    "    'new-york': '<city>',\n",
    "    'tokyo': '<city>',\n",
    "    'paris': '<city>',\n",
    "    'donald': '<person>',\n",
    "    'trump': '<person>',\n",
    "    'hakim': '<person>',\n",
    "    'andrew': '<person>',\n",
    "    'france': '<country>',\n",
    "    'mexico': '<country>',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define some helper functions to de/tokenize words and create the vocabs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<city>': 1, '<country>': 2, '<person>': 3, '<start>': 4, '<end>': 5, '<uknown>': 6, 'i': 7, 'love': 8, 'you': 9, 'what': 10, 'is': 11, 'your': 12, 'name': 13, '?': 14, 'how': 15, 'old': 16, 'are': 17, 'why': 18, 'the': 19, 'world': 20, 'so': 21, 'dumb': 22, 'found': 23, 'dog': 24, 'he': 25, 'died': 26, 'kissed': 27, 'him': 28, 'a': 29, 'beautiful': 30, 'city': 31, 'doing': 32, 'lost': 33, 'hope': 34, 'worst': 35, 'president': 36, 'do': 37, 'feel': 38, 'bad': 39, 'an': 40, 'apple': 41, 'at': 42, 'my': 43, 'desk': 44, 'broke': 45, 'mug': 46, 'hate': 47, 'me': 48}\n",
      "---\n",
      "{'<pad>': 0, '<city>': 1, '<country>': 2, '<person>': 3, '<start>': 4, '<end>': 5, '<uknown>': 6, 'je': 7, \"t'aime\": 8, 'comment': 9, 'tu': 10, \"t'appele\": 11, '?': 12, 'quel': 13, 'age': 14, 'as': 15, 'pourquoi': 16, 'le': 17, 'monde': 18, 'est': 19, 'si': 20, 'idiot': 21, \"j'ai\": 22, 'trouve': 23, 'chien': 24, 'il': 25, 'mort': 26, \"l'ai\": 27, 'embrasse': 28, 'une': 29, 'belle': 30, 'ville': 31, 'que': 32, 'ce': 33, 'fait': 34, 'perdu': 35, \"l'espoir\": 36, 'pire': 37, 'president': 38, 'te': 39, 'sens': 40, 'mauvais': 41, 'pomme': 42, 'sur': 43, 'mon': 44, 'bureau': 45, \"t'as\": 46, 'casse': 47, 'mug': 48, 'me': 49, 'deteste': 50}\n",
      "---\n",
      "12\n",
      "---\n",
      "[7, 8, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "class TranslationVocab:\n",
    "    def __init__(self):\n",
    "        self.ov = self.generate_empty_vocab() # original_vocab\n",
    "        self.tv = self.generate_empty_vocab() # target_vocab\n",
    "        self.ov_inv = {}\n",
    "        self.tv_inv = {}\n",
    "        self.ov_size = len(self.ov)\n",
    "        self.tv_size = len(self.tv)\n",
    "        self.o_max = 0 # original phrases data maximum sequence\n",
    "        self.t_max = 0 # target phrases data maximum sequence\n",
    "\n",
    "    def load_from_dataset(self, dataset):\n",
    "        # dataset is a list of lists, each inner list holds 2 strings ['original', 'translation']\n",
    "        for sample in dataset:\n",
    "            [original, target] = sample\n",
    "            o_splitted = original.lower().strip().split(' ')\n",
    "            t_splitted = target.lower().strip().split(' ')\n",
    "            \n",
    "            # loop over original phrase\n",
    "            for word in o_splitted:\n",
    "                self.insert_word_to_vocab(self.ov, word, self.ov_size)\n",
    "\n",
    "            # loop over target phrase\n",
    "            for word in t_splitted:\n",
    "                self.insert_word_to_vocab(self.tv, word, self.tv_size, is_target = True)\n",
    "                \n",
    "            # update max-lengths\n",
    "            self.o_max = max(len(o_splitted), self.o_max)\n",
    "            self.t_max = max(len(t_splitted) + 2, self.t_max) # +2 for <start> & <end> tokens\n",
    "            \n",
    "        # assign inverse map dicos\n",
    "        self.ov_inv = {v:k for k,v in self.ov.items()}\n",
    "        self.tv_inv = {v:k for k,v in self.tv.items()}\n",
    "        return self\n",
    "            \n",
    "    def get_token_of(self, word, is_target = False): # get orginal-vocab token of\n",
    "        vocab = self.ov\n",
    "        if is_target:\n",
    "            vocab = self.tv\n",
    "        if word in vocab:\n",
    "            return vocab[word]\n",
    "        if word in custom_words:\n",
    "            return vocab[ custom_words[word] ]\n",
    "        return vocab['<unknown>']\n",
    "\n",
    "    def tokenize_phrase(self, phrase, is_target = False, transformation = lambda x : '<start> ' + x + ' <end>'):\n",
    "        phrase = phrase.lower().strip()\n",
    "        if transformation is not None:\n",
    "            phrase = transformation(phrase)\n",
    "        tokenized = []\n",
    "        for word in phrase.split(' '):\n",
    "            tokenized.append(self.get_token_of(word, is_target))\n",
    "        # pad to max_len\n",
    "        max_len = max(self.o_max, self.t_max)\n",
    "#         if is_target:\n",
    "#             max_len = self.t_max\n",
    "        padding = [self.ov['<pad>']] * (max_len - len(tokenized))\n",
    "        if len(padding) > 0:\n",
    "            return tokenized + padding\n",
    "        return tokenized\n",
    "                \n",
    "    def tokenize_dataset(self, dataset): # return numpy arrays\n",
    "        X = []\n",
    "        y = []\n",
    "        for sample in dataset:\n",
    "            [original, target] = sample\n",
    "            X.append(np.array(self.tokenize_phrase(original, transformation = None), dtype=np.int32))\n",
    "            y.append(np.array(self.tokenize_phrase(target, is_target = True), dtype=np.int32))\n",
    "        return np.array(X), np.array(y)\n",
    "            \n",
    "            \n",
    "    def insert_word_to_vocab(self, vocab, word, index, is_target = False):\n",
    "        if not word in vocab:\n",
    "            if word not in custom_words:\n",
    "                vocab[word] = index\n",
    "                if is_target:\n",
    "                    self.tv_size += 1\n",
    "                else:\n",
    "                    self.ov_size += 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_empty_vocab(): # later to be loaded from a file\n",
    "        return {\n",
    "            '<pad>': 0,\n",
    "            '<city>': 1,\n",
    "            '<country>': 2,\n",
    "            '<person>': 3,\n",
    "            '<start>': 4,\n",
    "            '<end>': 5,\n",
    "            '<uknown>': 6,\n",
    "        }\n",
    "\n",
    "translation_vocab = TranslationVocab().load_from_dataset(dataset)\n",
    "print(translation_vocab.ov)\n",
    "print('---')\n",
    "print(translation_vocab.tv)\n",
    "print('---')\n",
    "print(translation_vocab.get_token_of('?', True))            \n",
    "print('---')\n",
    "print(translation_vocab.tokenize_phrase('I love Paris', is_target = False, transformation = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training input shape = (15, 9, 49)\n",
      "training output shape = (15, 9, 51)\n",
      "decoder input shape = (15, 9, 51)\n"
     ]
    }
   ],
   "source": [
    "X, decoder_inputs = translation_vocab.tokenize_dataset(dataset)\n",
    "y = decoder_inputs.copy()[:, 1:]\n",
    "y = np.insert(y, -1, [translation_vocab.ov['<pad>']], axis=1)\n",
    "X, y, decoder_inputs = one_hot(X, num_classes=translation_vocab.ov_size), one_hot(y, num_classes=translation_vocab.tv_size), one_hot(decoder_inputs, num_classes=translation_vocab.tv_size)\n",
    "# X, y = np.expand_dims(X, 2), np.expand_dims(y, 2) # to get a n_samples x Sequence-Length x 1 shape\n",
    "print(f'training input shape = {X.shape}')\n",
    "print(f'training output shape = {y.shape}')\n",
    "print(f'decoder input shape = {decoder_inputs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(n_hidden):\n",
    "    encoder_inputs = keras.layers.Input(shape=(None, 49))\n",
    "    _, h, c = keras.layers.LSTM(n_hidden, return_state=True, name='encoder_lstm')(encoder_inputs)\n",
    "    \n",
    "    decoder_inputs = keras.layers.Input(shape=(None, 51))\n",
    "    decoder_outputs, _, _ = keras.layers.LSTM(n_hidden, return_state = True, return_sequences=True, name='decoder_lstm')(decoder_inputs, initial_state=[h, c])\n",
    "    outputs = keras.layers.Dense(translation_vocab.tv_size, activation='softmax', name='decoder_dense')(decoder_outputs)\n",
    "    \n",
    "    model = keras.models.Model([encoder_inputs, decoder_inputs], outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_50 (InputLayer)           [(None, None, 49)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_51 (InputLayer)           [(None, None, 51)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, 32), (None,  10496       input_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 32), ( 10752       input_51[0][0]                   \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 51)     1683        decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 22,931\n",
      "Trainable params: 22,931\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = seq2seq_model(32)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1028 - acc: 1.0000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1023 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1018 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1013 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1008 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4b3b3d7df0>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X, decoder_inputs], y, verbose=1, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32)\n",
      "(1, 32)\n"
     ]
    }
   ],
   "source": [
    "enc_in = keras.layers.Input(shape=(None, 49))\n",
    "_, state_h, state_c = keras.layers.LSTM(32, return_state=True, name='enc_lstm')(enc_in)\n",
    "encoder = keras.models.Model(enc_in, [state_h, state_c])\n",
    "encoder.get_layer('enc_lstm').set_weights(model.get_layer('encoder_lstm').get_weights())\n",
    "\n",
    "def encode_phrase(phrase):\n",
    "    tokenized = translation_vocab.tokenize_phrase(phrase)\n",
    "    tokenized = np.array(tokenized)\n",
    "    tokenized = one_hot(tokenized, num_classes=translation_vocab.ov_size)\n",
    "    tokenized = np.expand_dims(tokenized, axis=0)\n",
    "    hidden, cell = encoder.predict(tokenized)\n",
    "    return hidden, cell\n",
    "\n",
    "h, c = encode_phrase('I love you')\n",
    "print(h.shape)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_in = keras.layers.Input(shape=(None, 51))\n",
    "input_h, input_c = keras.layers.Input(shape=(32, )), keras.layers.Input(shape=(32, ))\n",
    "\n",
    "decoder_outputs, state_h, state_c = keras.layers.LSTM(32, return_state = True, name='dec_lstm')(dec_in, initial_state=[input_h, input_c])\n",
    "outputs = keras.layers.Dense(translation_vocab.tv_size, activation='softmax', name='dec_dense')(decoder_outputs)\n",
    "decoder = keras.models.Model([dec_in, input_h, input_c], [outputs, state_h, state_c])\n",
    "\n",
    "# set weights\n",
    "decoder.get_layer('dec_lstm').set_weights(model.get_layer('decoder_lstm').get_weights())\n",
    "decoder.get_layer('dec_dense').set_weights(model.get_layer('decoder_dense').get_weights())\n",
    "\n",
    "def decode_states(h, c, in_token = '<start>'):\n",
    "    start_token = [translation_vocab.tv[in_token]]\n",
    "    start_token = np.array(start_token)\n",
    "    start_token = one_hot(start_token, num_classes=translation_vocab.tv_size)\n",
    "    start_token = np.expand_dims(start_token, axis=0)\n",
    "    out_word, h, c = decoder.predict([start_token, h, c])\n",
    "    out_word = translation_vocab.tv_inv[out_word.argmax()]\n",
    "    if out_word in ['<pad>', '<end>']:\n",
    "        return ''\n",
    "    return out_word + ' ' + decode_states(h, c, out_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(english_phrase):\n",
    "    h, c = encode_phrase(english_phrase)\n",
    "    return decode_states(h, c, in_token = '<start>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je t'aime \n",
      "je t'aime \n"
     ]
    }
   ],
   "source": [
    "print(translate('I love you'))\n",
    "print(translate('He died I love you'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
